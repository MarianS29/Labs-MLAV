{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58b2fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Lab 2\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa8fea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model (feed-forward)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364fc04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MNIST_train():\n",
    "    \n",
    "    mnist_train_data = np.zeros([60000,784]) # fiecare linie este o img 28x28\n",
    "    mnist_train_labels = np.zeros(60000)\n",
    "    \n",
    "    # ~ url = 'https://github.com/golbin/TensorFlow-MNIST/tree/master/mnist/data/train-images-idx3-ubyte.gz'\n",
    "    # ~ r = requests.get(url, allow_redirects=True)\n",
    "    # ~ open('train-images-idx3-ubyte.gz', 'wb').write(r.content)\n",
    "    \n",
    "    # ~ url = 'https://github.com/golbin/TensorFlow-MNIST/tree/master/mnist/datatrain-images-idx3-ubyte.gz'\n",
    "    # ~ r = requests.get(url, allow_redirects=True)\n",
    "    # ~ open('train-labels.idx1-ubyte.gz', 'wb').write(r.content)\n",
    "    \n",
    "    f = open('../../t10k-images.idx3-ubyte','r',encoding = 'latin-1')   \n",
    "    g = open('../../t10k-labels.idx1-ubyte','r',encoding = 'latin-1')\n",
    "    \n",
    "    byte = f.read(16) #4 bytes magic number, 4 bytes nr imag, 4 bytes nr linii, 4 bytes nr coloane\n",
    "    byte_label = g.read(8) #4 bytes magic number, 4 bytes nr labels\n",
    "    \n",
    "    mnist_train_data = np.fromfile(f,dtype=np.uint8).reshape(60000,784)\n",
    "    mnist_train_labels = np.fromfile(g,dtype=np.uint8)\n",
    "        \n",
    "    # Conversii pentru a se potrivi cu procesul de antrenare    \n",
    "    mnist_train_data = mnist_train_data.astype(np.float32)\n",
    "    mnist_train_labels = mnist_train_labels.astype(np.int64)\n",
    "        \n",
    "    return mnist_train_data, mnist_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be62fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MNIST_test():\n",
    "    \n",
    "    mnist_test_data = np.zeros([10000,784])\n",
    "    mnist_test_labels = np.zeros(10000)\n",
    "    \n",
    "    f = open('../../t10k-images.idx3-ubyte','r',encoding = 'latin-1')\n",
    "    g = open('../../t10k-labels.idx1-ubyte','r',encoding = 'latin-1')\n",
    "    \n",
    "    byte = f.read(16) #4 bytes magic number, 4 bytes nr imag, 4 bytes nr linii, 4 bytes nr coloane\n",
    "    byte_label = g.read(8) #4 bytes magic number, 4 bytes nr labels\n",
    "    \n",
    "    mnist_test_data = np.fromfile(f,dtype=np.uint8).reshape(10000,784)\n",
    "    mnist_test_labels = np.fromfile(g,dtype=np.uint8)\n",
    "    \n",
    "    # Conversii pentru a se potrivi cu procesul de testare    \n",
    "    mnist_test_data = mnist_test_data.astype(np.float32)\n",
    "    mnist_test_labels = mnist_test_labels.astype(np.int64)        \n",
    "    \n",
    "    return mnist_test_data, mnist_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe85496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Retea_MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, nr_neuroni_input, nr_neuroni_hidden, nr_clase):\n",
    "        \n",
    "        # Pentru a putea folosi mai departe reteaua, este recomandata mostenirea\n",
    "        # clasei de baza nn.Module\n",
    "        super(Retea_MLP,self).__init__()\n",
    "        \n",
    "        # Definirea ponderilor si a deplasamentelor din stratul ascuns\n",
    "        self.w_h = torch.randn(nr_neuroni_input, nr_neuroni_hidden, dtype = torch.float, requires_grad=True)\n",
    "        self.b_h = torch.randn(nr_neuroni_hidden, dtype = torch.float, requires_grad=True)\n",
    "        \n",
    "        # Definirea ponderilor si a deplasamentelor din stratul de iesire\n",
    "        self.w_o = torch.randn(nr_neuroni_hidden, nr_clase, dtype = torch.float, requires_grad=True)\n",
    "        self.b_o = torch.randn(nr_clase, dtype = torch.float, requires_grad=True)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    \n",
    "    # Se aduna toate variabilele antrenabile intr-o lista, pentru a putea face referire rapida la ele\n",
    "    def parameters(self):\n",
    "        return [self.w_h, self.b_h, self.w_o, self.b_o]\n",
    "    \n",
    "    def forward(self,input_batch):\n",
    "        # Intr-un MLP, intrarea este sub forma unui vector, deci un batch\n",
    "        # este o matrice de dimensiunea nr_esantioane_batch x dimensiune esantion\n",
    "        input_batch = torch.from_numpy(input_batch)\n",
    "        self.hidden = torch.mm(input_batch, self.w_h) + self.b_h\n",
    "\n",
    "        self.hidden = self.relu(self.hidden)\n",
    "        \n",
    "        out = torch.mm(self.hidden, self.w_o) + self.b_o\n",
    "\n",
    "        # out = softmax(out) - se poate si fara (softmax este inclus in CrossEntropyLoss)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "020c5341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiem reteaua\n",
    "mlp = Retea_MLP(28*28,1000,10)\n",
    "\n",
    "# Specificarea functiei loss\n",
    "loss_function = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# Specificarea optimizatorului\n",
    "optim = torch.optim.SGD(mlp.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "621ab362",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 7840000 into shape (60000,784)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Bucla de invatare\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m train_data, train_labels = \u001b[43mget_MNIST_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m batch_size = \u001b[32m128\u001b[39m \u001b[38;5;66;03m# Se poate si mai mult in cazul curent, dar este o valoare frecventa\u001b[39;00m\n\u001b[32m      6\u001b[39m nr_epoci = \u001b[32m15\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mget_MNIST_train\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m byte = f.read(\u001b[32m16\u001b[39m) \u001b[38;5;66;03m#4 bytes magic number, 4 bytes nr imag, 4 bytes nr linii, 4 bytes nr coloane\u001b[39;00m\n\u001b[32m     18\u001b[39m byte_label = g.read(\u001b[32m8\u001b[39m) \u001b[38;5;66;03m#4 bytes magic number, 4 bytes nr labels\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m mnist_train_data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m60000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m784\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m mnist_train_labels = np.fromfile(g,dtype=np.uint8)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Conversii pentru a se potrivi cu procesul de antrenare    \u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: cannot reshape array of size 7840000 into shape (60000,784)"
     ]
    }
   ],
   "source": [
    "# Bucla de invatare\n",
    "import numpy as np\n",
    "\n",
    "train_data, train_labels = get_MNIST_train()\n",
    "batch_size = 128 # Se poate si mai mult in cazul curent, dar este o valoare frecventa\n",
    "nr_epoci = 15\n",
    "nr_iteratii = train_data.shape[0] // batch_size # Din simplitate, vom ignora ultimul batch, daca este incomplet\n",
    "\n",
    "\n",
    "for ep in range(nr_epoci):\n",
    "    predictii = []\n",
    "    etichete = []\n",
    "\n",
    "    for it in range(nr_iteratii):\n",
    "        # Luam urmatoarele <batch_size> esantioane si etichete\n",
    "        batch_data = train_data[it*batch_size : it*batch_size+batch_size, :]\n",
    "        batch_labels = train_labels[it*batch_size : it*batch_size+batch_size]\n",
    "        # Se calculeaza predictia retelei pentru datele curente (forward pass/ propagare inainte)\n",
    "        current_predict = mlp.forward(batch_data)\n",
    "\n",
    "        # Se calculeaza valoarea momentana a functiei loss\n",
    "        loss = loss_function(current_predict, torch.from_numpy(batch_labels)) \n",
    "        \n",
    "        # Se memoreaza predictiile si etichetele aferente batch-ului actual (pentru calculul acuratetii)\n",
    "        current_predict = np.argmax(current_predict.detach().numpy(), axis=1)\n",
    "        predictii = np.concatenate((predictii,current_predict))\n",
    "        etichete = np.concatenate((etichete,batch_labels))\n",
    "        \n",
    "        # Antrenarea propriu-zisa\n",
    "        \n",
    "            # 1. Se sterg toti gradientii calculati anteriori, pentru toate variabilele antrenabile\n",
    "            # deoarece, metoda <backward> acumuleaza noile valori, in loc sa le inlocuiasca.\n",
    "        optim.zero_grad()\n",
    "            # 2. Calculul tuturor gradientilor. Backpropagation\n",
    "        loss.backward()\n",
    "            # 3. Actualizarea tuturor ponderilor, pe baza gradientilor.\n",
    "        optim.step()\n",
    "        \n",
    "        \n",
    "\n",
    "    # Calculam acuratetea\n",
    "    acc = np.sum(predictii==etichete)/len(predictii)\n",
    "    print( 'Acuratetea la epoca {} este {}%'.format(ep+1,acc*100) )\n",
    "\n",
    "    # Se genereaza o permutare noua a tuturor esantioanelor si etichetelor corespunzatoare\n",
    "    perm = np.random.permutation(train_data.shape[0])\n",
    "    train_data = train_data[perm,:]\n",
    "    train_labels = train_labels[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0bb5889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratetea la test este 8.55%\n"
     ]
    }
   ],
   "source": [
    "# ~ 8. Testarea retelei\n",
    "\n",
    "# ~ Odata terminata antrenarea retelei, putem testa pe un set de date noi. Veti observa ca structura de la bucla de antrenare ne va ajuta in continuare:\n",
    "\n",
    "test_data, test_labels = get_MNIST_test()\n",
    "batch_size_test = 100 # pentru usurinta, ca sa testam toate etichetele alegem un divizor al numarului de date de test\n",
    "nr_iteratii_test = test_data.shape[0] // batch_size_test\n",
    "    \n",
    "predictii = []\n",
    "for it in range(nr_iteratii_test):\n",
    "    batch_data = test_data[it*batch_size_test : it*batch_size_test+batch_size_test, :]\n",
    "    batch_labels = test_labels[it*batch_size_test : it*batch_size_test+batch_size_test]\n",
    "\n",
    "    current_predict = mlp.forward(batch_data)\n",
    "    current_predict = np.argmax(current_predict.detach().numpy(),axis=1)\n",
    "    predictii = np.concatenate((predictii,current_predict))\n",
    "\n",
    "acc = np.sum(predictii==test_labels)/len(predictii)\n",
    "print( 'Acuratetea la test este {}%'.format(acc*100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "247486fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplificare\n",
    "\n",
    "# Modulul nn contine o multitudine de elemente\n",
    "# esentiale construirii unei retele neuronale\n",
    "import torch.nn as nn\n",
    "\n",
    "class Retea_MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, nr_neuroni_input, nr_neuroni_hidden, nr_clase):\n",
    "        \n",
    "        # Pentru a putea folosi mai departe reteaua, este recomandata mostenirea\n",
    "        # clasei de baza nn.Module\n",
    "        super(Retea_MLP,self).__init__()\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(nr_neuroni_input, nr_neuroni_hidden)\n",
    "        self.out_layer = nn.Linear(nr_neuroni_hidden, nr_clase)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,input_batch):\n",
    "        # Intr-un MLP, intrarea este sub forma unui vector, deci un batch\n",
    "        # este o matrice de dimensiunea nr_esantioane_batch x dimensiune esantion\n",
    "        input_batch = torch.from_numpy(input_batch)\n",
    "        hidden = self.hidden_layer(input_batch)\n",
    "\n",
    "        self.hidden = self.relu(self.hidden)\n",
    "        \n",
    "        out = self.out_layer(hidden)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "# Instantiem reteaua\n",
    "mlp = Retea_MLP(28*28,1000,10)\n",
    "\n",
    "# Specificarea functiei loss\n",
    "loss_function = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# Specificarea optimizatorului\n",
    "optim = torch.optim.SGD(mlp.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c40eb66e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 7840000 into shape (60000,784)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_data, train_labels = \u001b[43mget_MNIST_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m batch_size = \u001b[32m128\u001b[39m \u001b[38;5;66;03m# Se poate si mai mult in cazul curent, dar este o valoare frecventa\u001b[39;00m\n\u001b[32m      3\u001b[39m nr_epoci = \u001b[32m15\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mget_MNIST_train\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m byte = f.read(\u001b[32m16\u001b[39m) \u001b[38;5;66;03m#4 bytes magic number, 4 bytes nr imag, 4 bytes nr linii, 4 bytes nr coloane\u001b[39;00m\n\u001b[32m     18\u001b[39m byte_label = g.read(\u001b[32m8\u001b[39m) \u001b[38;5;66;03m#4 bytes magic number, 4 bytes nr labels\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m mnist_train_data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m60000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m784\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m mnist_train_labels = np.fromfile(g,dtype=np.uint8)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Conversii pentru a se potrivi cu procesul de antrenare    \u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: cannot reshape array of size 7840000 into shape (60000,784)"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = get_MNIST_train()\n",
    "batch_size = 128 # Se poate si mai mult in cazul curent, dar este o valoare frecventa\n",
    "nr_epoci = 15\n",
    "nr_iteratii = train_data.shape[0] // batch_size # Din simplitate, vom ignora ultimul batch, daca este incomplet\n",
    "\n",
    "\n",
    "for ep in range(nr_epoci):\n",
    "    predictii = []\n",
    "    etichete = []\n",
    "\n",
    "    for it in range(nr_iteratii):\n",
    "        # Luam urmatoarele <batch_size> esantioane si etichete\n",
    "        batch_data = train_data[it*batch_size : it*batch_size+batch_size, :]\n",
    "        batch_labels = train_labels[it*batch_size : it*batch_size+batch_size]\n",
    "        # Se calculeaza predictia retelei pentru datele curente (forward pass/ propagare inainte)\n",
    "        current_predict = mlp.forward(batch_data)\n",
    "\n",
    "        # Se calculeaza valoarea momentana a functiei loss\n",
    "        loss = loss_function(current_predict, torch.from_numpy(batch_labels)) \n",
    "        \n",
    "        # Se memoreaza predictiile si etichetele aferente batch-ului actual (pentru calculul acuratetii)\n",
    "        current_predict = np.argmax(current_predict.detach().numpy(), axis=1)\n",
    "        predictii = np.concatenate((predictii,current_predict))\n",
    "        etichete = np.concatenate((etichete,batch_labels))\n",
    "        \n",
    "        # Antrenarea propriu-zisa\n",
    "        \n",
    "            # 1. Se sterg toti gradientii calculati anteriori, pentru toate variabilele antrenabile\n",
    "            # deoarece, metoda <backward> acumuleaza noile valori, in loc sa le inlocuiasca.\n",
    "        optim.zero_grad()\n",
    "            # 2. Calculul tuturor gradientilor. Backpropagation\n",
    "        loss.backward()\n",
    "            # 3. Actualizarea tuturor ponderilor, pe baza gradientilor.\n",
    "        optim.step()\n",
    "        \n",
    "        \n",
    "\n",
    "    # Calculam acuratetea\n",
    "    acc = np.sum(predictii==etichete)/len(predictii)\n",
    "    print( 'Acuratetea la epoca {} este {}%'.format(ep+1,acc*100) )\n",
    "\n",
    "    # Se genereaza o permutare noua a tuturor esantioanelor si etichetelor corespunzatoare\n",
    "    perm = np.random.permutation(train_data.shape[0])\n",
    "    train_data = train_data[perm,:]\n",
    "    train_labels = train_labels[perm]\n",
    "\n",
    "    test_data, test_labels = get_MNIST_test()\n",
    "    batch_size_test = 100 # pentru usurinta, ca sa testam toate etichetele alegem un divizor al numarului de date de test\n",
    "    nr_iteratii_test = test_data.shape[0] // batch_size_test\n",
    "        \n",
    "    predictii = []\n",
    "    for it in range(nr_iteratii_test):\n",
    "        batch_data = test_data[it*batch_size_test : it*batch_size_test+batch_size_test, :]\n",
    "        batch_labels = test_labels[it*batch_size_test : it*batch_size_test+batch_size_test]\n",
    "\n",
    "        current_predict = mlp.forward(batch_data)\n",
    "        current_predict = np.argmax(current_predict.detach().numpy(),axis=1)\n",
    "        predictii = np.concatenate((predictii,current_predict))\n",
    "\n",
    "    acc = np.sum(predictii==test_labels)/len(predictii)\n",
    "    print( 'Acuratetea la test este {}%'.format(acc*100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72dcc3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Retea_MLP(\n",
       "  (hidden_layer): Linear(in_features=784, out_features=1000, bias=True)\n",
       "  (out_layer): Linear(in_features=1000, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(mlp.state_dict(), './mlp_mnist.pt')\n",
    "\n",
    "# Descrierea arhitecturii\n",
    "mlp_antrenat = Retea_MLP(28*28,1000,10)\n",
    "\n",
    "# Se vor incarca ponderile calculate anterior.\n",
    "mlp_antrenat.load_state_dict(torch.load('./mlp_mnist.pt'))\n",
    "\n",
    "# In majoritatea cazurilor, trebuie mentionat faptul ca nu se mai antreneaza reteaua.\n",
    "# Anumite straturi au comporamente diferite la antrenare, fata de testare. In cazul\n",
    "# de fata, nu ar trebui sa fie necesara aceasta trecere in modul de inferenta.\n",
    "mlp_antrenat.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6165a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratetea la epoca 1 este 91.87032585470085%\n",
      "Acuratetea la epoca 2 este 96.4710202991453%\n",
      "Acuratetea la epoca 3 este 97.18716613247864%\n",
      "Acuratetea la epoca 4 este 97.11371527777779%\n",
      "Acuratetea la epoca 5 este 97.0836672008547%\n",
      "Acuratetea la epoca 6 este 97.05862713675214%\n",
      "Acuratetea la epoca 7 este 97.10536858974359%\n",
      "Acuratetea la epoca 8 este 97.53271901709401%\n",
      "Acuratetea la epoca 9 este 97.80815972222221%\n",
      "Acuratetea la epoca 10 este 97.71634615384616%\n",
      "Acuratetea la epoca 11 este 97.83653846153845%\n",
      "Acuratetea la epoca 12 este 97.83987713675214%\n",
      "Acuratetea la epoca 13 este 98.1620592948718%\n",
      "Acuratetea la epoca 14 este 98.29393696581197%\n",
      "Acuratetea la epoca 15 este 98.33733974358975%\n",
      "Acuratetea la test este 96.83%\n"
     ]
    }
   ],
   "source": [
    "def get_MNIST_train():\n",
    "    \n",
    "    mnist_train_data = np.zeros([60000,784])\n",
    "    mnist_train_labels = np.zeros(60000)\n",
    "    \n",
    "    f = open('../../train-images.idx3-ubyte','r',encoding = 'latin-1')\n",
    "    g = open('../../train-labels.idx1-ubyte','r',encoding = 'latin-1')\n",
    "    \n",
    "    byte = f.read(16) #4 bytes magic number, 4 bytes nr imag, 4 bytes nr linii, 4 bytes nr coloane\n",
    "    byte_label = g.read(8) #4 bytes magic number, 4 bytes nr labels\n",
    "    \n",
    "    mnist_train_data = np.fromfile(f,dtype=np.uint8).reshape(60000,784)\n",
    "    mnist_train_labels = np.fromfile(g,dtype=np.uint8)\n",
    "        \n",
    "    # Conversii pentru a se potrivi cu procesul de antrenare    \n",
    "    mnist_train_data = mnist_train_data.astype(np.float32)\n",
    "    mnist_train_labels = mnist_train_labels.astype(np.int64)\n",
    "        \n",
    "    return mnist_train_data, mnist_train_labels\n",
    "\n",
    "def get_MNIST_test():\n",
    "    \n",
    "    mnist_test_data = np.zeros([10000,784])\n",
    "    mnist_test_labels = np.zeros(10000)\n",
    "    \n",
    "    f = open('../../t10k-images.idx3-ubyte','r',encoding = 'latin-1')\n",
    "    g = open('../../t10k-labels.idx1-ubyte','r',encoding = 'latin-1')\n",
    "    \n",
    "    byte = f.read(16) #4 bytes magic number, 4 bytes nr imag, 4 bytes nr linii, 4 bytes nr coloane\n",
    "    byte_label = g.read(8) #4 bytes magic number, 4 bytes nr labels\n",
    "    \n",
    "    mnist_test_data = np.fromfile(f,dtype=np.uint8).reshape(10000,784)\n",
    "    mnist_test_labels = np.fromfile(g,dtype=np.uint8)\n",
    "    \n",
    "    # Conversii pentru a se potrivi cu procesul de testare    \n",
    "    mnist_test_data = mnist_test_data.astype(np.float32)\n",
    "    mnist_test_labels = mnist_test_labels.astype(np.int64)        \n",
    "    \n",
    "    return mnist_test_data, mnist_test_labels\n",
    "\n",
    "\n",
    "\n",
    "# Modulul nn contine o multitudine de elemente\n",
    "# esentiale construirii unei retele neuronale\n",
    "import torch.nn as nn\n",
    "\n",
    "class Retea_MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, nr_neuroni_input, nr_neuroni_hidden, nr_clase):\n",
    "        \n",
    "        # Pentru a putea folosi mai departe reteaua, este recomandata mostenirea\n",
    "        # clasei de baza nn.Module\n",
    "        super(Retea_MLP,self).__init__()\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(nr_neuroni_input, nr_neuroni_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_layer = nn.Linear(nr_neuroni_hidden, nr_clase)\n",
    "        \n",
    "    def forward(self,input_batch):\n",
    "        # Intr-un MLP, intrarea este sub forma unui vector, deci un batch\n",
    "        # este o matrice de dimensiunea nr_esantioane_batch x dimensiune esantion\n",
    "        input_batch = torch.from_numpy(input_batch)\n",
    "        hidden = self.relu(self.hidden_layer(input_batch))\n",
    "        out = self.out_layer(hidden)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "# Instantiem reteaua\n",
    "mlp = Retea_MLP(28*28,1000,10)\n",
    "\n",
    "# Specificarea functiei loss\n",
    "loss_function = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# Specificarea optimizatorului\n",
    "optim = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "\n",
    "train_data, train_labels = get_MNIST_train()\n",
    "batch_size = 128 # Se poate si mai mult in cazul curent, dar este o valoare frecventa\n",
    "nr_epoci = 15\n",
    "nr_iteratii = train_data.shape[0] // batch_size # Din simplitate, vom ignora ultimul batch, daca este incomplet\n",
    "\n",
    "\n",
    "for ep in range(nr_epoci):\n",
    "    predictii = []\n",
    "    etichete = []\n",
    "\n",
    "    for it in range(nr_iteratii):\n",
    "        # Luam urmatoarele <batch_size> esantioane si etichete\n",
    "        batch_data = train_data[it*batch_size : it*batch_size+batch_size, :]\n",
    "        batch_labels = train_labels[it*batch_size : it*batch_size+batch_size]\n",
    "        # Se calculeaza predictia retelei pentru datele curente (forward pass/ propagare inainte)\n",
    "        current_predict = mlp.forward(batch_data)\n",
    "\n",
    "        # Se calculeaza valoarea momentana a functiei loss\n",
    "        loss = loss_function(current_predict, torch.from_numpy(batch_labels)) \n",
    "        \n",
    "        # Se memoreaza predictiile si etichetele aferente batch-ului actual (pentru calculul acuratetii)\n",
    "        current_predict = np.argmax(current_predict.detach().numpy(), axis=1)\n",
    "        predictii = np.concatenate((predictii,current_predict))\n",
    "        etichete = np.concatenate((etichete,batch_labels))\n",
    "        \n",
    "        # Antrenarea propriu-zisa\n",
    "        \n",
    "            # 1. Se sterg toti gradientii calculati anteriori, pentru toate variabilele antrenabile\n",
    "            # deoarece, metoda <backward> acumuleaza noile valori, in loc sa le inlocuiasca.\n",
    "        optim.zero_grad()\n",
    "            # 2. Calculul tuturor gradientilor. Backpropagation\n",
    "        loss.backward()\n",
    "            # 3. Actualizarea tuturor ponderilor, pe baza gradientilor.\n",
    "        optim.step()\n",
    "        \n",
    "        \n",
    "\n",
    "    # Calculam acuratetea\n",
    "    acc = np.sum(predictii==etichete)/len(predictii)\n",
    "    print( 'Acuratetea la epoca {} este {}%'.format(ep+1,acc*100) )\n",
    "\n",
    "    # Se genereaza o permutare noua a tuturor esantioanelor si etichetelor corespunzatoare\n",
    "    perm = np.random.permutation(train_data.shape[0])\n",
    "    train_data = train_data[perm,:]\n",
    "    train_labels = train_labels[perm]\n",
    "    \n",
    "    \n",
    "    \n",
    "test_data, test_labels = get_MNIST_test()\n",
    "batch_size_test = 100 # pentru usurinta, ca sa testam toate etichetele alegem un divizor al numarului de date de test\n",
    "nr_iteratii_test = test_data.shape[0] // batch_size_test\n",
    "    \n",
    "predictii = []\n",
    "for it in range(nr_iteratii_test):\n",
    "    batch_data = test_data[it*batch_size_test : it*batch_size_test+batch_size_test, :]\n",
    "    batch_labels = test_labels[it*batch_size_test : it*batch_size_test+batch_size_test]\n",
    "\n",
    "    current_predict = mlp.forward(batch_data)\n",
    "    current_predict = np.argmax(current_predict.detach().numpy(),axis=1)\n",
    "    predictii = np.concatenate((predictii,current_predict))\n",
    "\n",
    "acc = np.sum(predictii==test_labels)/len(predictii)\n",
    "print( 'Acuratetea la test este {}%'.format(acc*100) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
