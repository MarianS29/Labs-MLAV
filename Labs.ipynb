{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc9a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from sklearn import datasets,ensemble,metrics\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f091104",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################### LAB 2+3 ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35d99fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MNIST_train():\n",
    "    \n",
    "    mnist_train_data = np.zeros([60000,784])\n",
    "    mnist_train_labels = np.zeros(60000)\n",
    "    \n",
    "    f = open('train-images.idx3-ubyte','r',encoding = 'latin-1')\n",
    "    g = open('train-labels.idx1-ubyte','r',encoding = 'latin-1')\n",
    "    \n",
    "    byte = f.read(16) #4 bytes magic number, 4 bytes nr imag, 4 bytes nr linii, 4 bytes nr coloane\n",
    "    byte_label = g.read(8) #4bytes magic number, 4 bytes nr labels\n",
    "    \n",
    "    mnist_train_data = np.fromfile(f,dtype=np.uint8).reshape(60000,784)\n",
    "    mnist_train_data = mnist_train_data.reshape(60000,1,28,28)\n",
    "    mnist_train_labels = np.fromfile(g,dtype=np.uint8)\n",
    "    \n",
    "    # Conversii pentru a se potrivi cu procesul de antrenare    \n",
    "    mnist_train_data = mnist_train_data.astype(np.float32)\n",
    "    mnist_train_labels = mnist_train_labels.astype(np.int64)\n",
    "    \n",
    "    return mnist_train_data, mnist_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f8876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MNIST_test():\n",
    "    \n",
    "    mnist_test_data = np.zeros([10000,784])\n",
    "    mnist_test_labels = np.zeros(10000)\n",
    "    \n",
    "    f = open('t10k-images.idx3-ubyte','r',encoding = 'latin-1')\n",
    "    g = open('t10k-labels.idx1-ubyte','r',encoding = 'latin-1')\n",
    "    \n",
    "    byte = f.read(16) #4 bytes magic number, 4 bytes nr imag, 4 bytes nr linii, 4 bytes nr coloane\n",
    "    byte_label = g.read(8) #4bytes magic number, 4 bytes nr labels\n",
    "    \n",
    "    mnist_test_data = np.fromfile(f,dtype=np.uint8).reshape(10000,784)\n",
    "    mnist_test_data = mnist_test_data.reshape(10000,1,28,28)\n",
    "    mnist_test_labels = np.fromfile(g,dtype=np.uint8)\n",
    "        \n",
    "    # Conversii pentru a se potrivi cu procesul de antrenare    \n",
    "    mnist_test_data = mnist_test_data.astype(np.float32)\n",
    "    mnist_test_labels = mnist_test_labels.astype(np.int64)\n",
    "    \n",
    "    return mnist_test_data, mnist_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcbd3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP cu PyTorch\n",
    "\n",
    "class Retea_MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, nr_neuroni_input, nr_neuroni_hidden, nr_clase):\n",
    "        \n",
    "        # Pentru a putea folosi mai departe reteaua, este recomandata mostenirea\n",
    "        # clasei de baza nn.Module\n",
    "        super(Retea_MLP,self).__init__()\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(nr_neuroni_input, nr_neuroni_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_layer = nn.Linear(nr_neuroni_hidden, nr_clase)\n",
    "        \n",
    "    def forward(self,input_batch):\n",
    "        # Intr-un MLP, intrarea este sub forma unui vector, deci un batch\n",
    "        # este o matrice de dimensiunea nr_esantioane_batch x dimensiune esantion\n",
    "        input_batch = torch.from_numpy(input_batch)\n",
    "        \n",
    "        # Flatten the input: (batch_size, 1, 28, 28) -> (batch_size, 784)\n",
    "        input_batch = input_batch.view(input_batch.shape[0], -1)\n",
    "        hidden = self.relu(self.hidden_layer(input_batch))\n",
    "        out = self.out_layer(hidden)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91989c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "\n",
    "class Retea_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Definirea straturilor CNN\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Definirea stratului fully connected\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Definirea propagarii forward\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flattening\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d4a2e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratetea la epoca 1 este 91.60823985042735%\n",
      "Acuratetea la epoca 2 este 96.60790598290599%\n",
      "Acuratetea la epoca 3 este 97.04026442307693%\n",
      "Acuratetea la epoca 4 este 97.27063301282051%\n",
      "Acuratetea la epoca 5 este 97.21554487179486%\n",
      "Acuratetea la epoca 6 este 97.2255608974359%\n",
      "Acuratetea la epoca 7 este 97.0302483974359%\n",
      "Acuratetea la epoca 8 este 97.21554487179486%\n",
      "Acuratetea la epoca 9 este 97.29400373931624%\n",
      "Acuratetea la epoca 10 este 97.61785523504274%\n",
      "Acuratetea la epoca 11 este 97.96507745726495%\n",
      "Acuratetea la epoca 12 este 97.9400373931624%\n",
      "Acuratetea la epoca 13 este 98.00347222222221%\n",
      "Acuratetea la epoca 14 este 98.17040598290599%\n",
      "Acuratetea la epoca 15 este 98.19377670940172%\n",
      "Acuratetea la test este 96.38%\n"
     ]
    }
   ],
   "source": [
    "# Train MLP pe MNIST\n",
    "    \n",
    "# Instantiem reteaua\n",
    "mlp = Retea_MLP(28*28,1000,10)\n",
    "\n",
    "# Specificarea functiei loss\n",
    "loss_function = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# Specificarea optimizatorului\n",
    "optim = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "\n",
    "train_data, train_labels = get_MNIST_train()\n",
    "batch_size = 128 # Se poate si mai mult in cazul curent, dar este o valoare frecventa\n",
    "nr_epoci = 15\n",
    "nr_iteratii = train_data.shape[0] // batch_size # Din simplitate, vom ignora ultimul batch, daca este incomplet\n",
    "\n",
    "\n",
    "for ep in range(nr_epoci):\n",
    "    predictii = []\n",
    "    etichete = []\n",
    "\n",
    "    for it in range(nr_iteratii):\n",
    "        # Luam urmatoarele <batch_size> esantioane si etichete\n",
    "        batch_data = train_data[it*batch_size : it*batch_size+batch_size, :]\n",
    "        batch_labels = train_labels[it*batch_size : it*batch_size+batch_size]\n",
    "        # Se calculeaza predictia retelei pentru datele curente (forward pass/ propagare inainte)\n",
    "        current_predict = mlp.forward(batch_data)\n",
    "\n",
    "        # Se calculeaza valoarea momentana a functiei loss\n",
    "        loss = loss_function(current_predict, torch.from_numpy(batch_labels)) \n",
    "        \n",
    "        # Se memoreaza predictiile si etichetele aferente batch-ului actual (pentru calculul acuratetii)\n",
    "        current_predict = np.argmax(current_predict.detach().numpy(), axis=1)\n",
    "        predictii = np.concatenate((predictii,current_predict))\n",
    "        etichete = np.concatenate((etichete,batch_labels))\n",
    "        \n",
    "        # Antrenarea propriu-zisa\n",
    "        \n",
    "            # 1. Se sterg toti gradientii calculati anteriori, pentru toate variabilele antrenabile\n",
    "            # deoarece, metoda <backward> acumuleaza noile valori, in loc sa le inlocuiasca.\n",
    "        optim.zero_grad()\n",
    "            # 2. Calculul tuturor gradientilor. Backpropagation\n",
    "        loss.backward()\n",
    "            # 3. Actualizarea tuturor ponderilor, pe baza gradientilor.\n",
    "        optim.step()\n",
    "        \n",
    "        \n",
    "\n",
    "    # Calculam acuratetea\n",
    "    acc = np.sum(predictii==etichete)/len(predictii)\n",
    "    print( 'Acuratetea la epoca {} este {}%'.format(ep+1,acc*100) )\n",
    "\n",
    "    # Se genereaza o permutare noua a tuturor esantioanelor si etichetelor corespunzatoare\n",
    "    perm = np.random.permutation(train_data.shape[0])\n",
    "    train_data = train_data[perm,:]\n",
    "    train_labels = train_labels[perm]\n",
    "    \n",
    "    \n",
    "    \n",
    "test_data, test_labels = get_MNIST_test()\n",
    "batch_size_test = 100 # pentru usurinta, ca sa testam toate etichetele alegem un divizor al numarului de date de test\n",
    "nr_iteratii_test = test_data.shape[0] // batch_size_test\n",
    "    \n",
    "predictii = []\n",
    "for it in range(nr_iteratii_test):\n",
    "    batch_data = test_data[it*batch_size_test : it*batch_size_test+batch_size_test, :]\n",
    "    batch_labels = test_labels[it*batch_size_test : it*batch_size_test+batch_size_test]\n",
    "\n",
    "    current_predict = mlp.forward(batch_data)\n",
    "    current_predict = np.argmax(current_predict.detach().numpy(),axis=1)\n",
    "    predictii = np.concatenate((predictii,current_predict))\n",
    "\n",
    "acc = np.sum(predictii==test_labels)/len(predictii)\n",
    "print( 'Acuratetea la test este {}%'.format(acc*100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e3309a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratetea la epoca 1 este 89.91166666666666%\n",
      "Acuratetea la epoca 2 este 96.47166666666666%\n",
      "Acuratetea la epoca 3 este 97.47333333333333%\n",
      "Acuratetea la epoca 4 este 97.93166666666666%\n",
      "Acuratetea la epoca 5 este 98.25500000000001%\n",
      "Acuratetea la epoca 6 este 98.46166666666667%\n",
      "Acuratetea la epoca 7 este 98.67166666666667%\n",
      "Acuratetea la epoca 8 este 98.81333333333333%\n",
      "Acuratetea la epoca 9 este 98.91499999999999%\n",
      "Acuratetea la epoca 10 este 99.02166666666666%\n",
      "Acuratetea la epoca 11 este 99.11666666666666%\n",
      "Acuratetea la epoca 12 este 99.19%\n",
      "Acuratetea la epoca 13 este 99.24666666666667%\n",
      "Acuratetea la epoca 14 este 99.32666666666667%\n",
      "Acuratetea la epoca 15 este 99.39833333333333%\n"
     ]
    }
   ],
   "source": [
    "# Train CNN with MNIST Dataset\n",
    "\n",
    "class DatasetMNIST(Dataset):\n",
    "    def __init__(self):\n",
    "\n",
    "        mnist_train_data,mnist_train_labels = get_MNIST_train()\n",
    "\n",
    "        self.data = mnist_train_data\n",
    "        self.labels = mnist_train_labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_point = self.data[index, ...]\n",
    "        label_point = self.labels[index, ...]\n",
    "\n",
    "        return {'data': data_point, 'label': label_point}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "###\n",
    "\n",
    "mnistdataset = DatasetMNIST()\n",
    "mnistdataloader = DataLoader(mnistdataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "loss_function = nn.CrossEntropyLoss(reduction='sum')\n",
    "simple_CNN = Retea_CNN().to(\"cuda\")\n",
    "optim = torch.optim.SGD(simple_CNN.parameters(), lr=1e-5)\n",
    "\n",
    "for ep in range(15):\n",
    "\n",
    "    predictii = []\n",
    "    etichete = []\n",
    "\n",
    "    for batch in mnistdataloader:\n",
    "        data = batch['data'].to(\"cuda\")\n",
    "        label = batch['label'].to(\"cuda\")\n",
    "\n",
    "        # print(data.shape)\n",
    "        # print(label.shape)\n",
    "\n",
    "        # data_as_image = data[0, ...].cpu().detach().numpy().squeeze()\n",
    "        # print(data_as_image.shape)\n",
    "\n",
    "        # plt.imshow(data_as_image, cmap='gray')\n",
    "        # continue\n",
    "\n",
    "        current_predict = simple_CNN.forward(data)\n",
    "\n",
    "        # Se calculeaza valoarea momentana a functiei loss\n",
    "        loss = loss_function(current_predict, label) \n",
    "        \n",
    "        # Se memoreaza predictiile si etichetele aferente batch-ului actual (pentru calculul acuratetii)\n",
    "        current_predict = np.argmax(current_predict.cpu().detach().numpy(), axis=1)\n",
    "        label = label.cpu().detach().numpy()\n",
    "\n",
    "        predictii = np.concatenate((predictii,current_predict))\n",
    "        etichete = np.concatenate((etichete,label))\n",
    "\n",
    "        optim.zero_grad()\n",
    "            # 2. Calculul tuturor gradientilor. Backpropagation\n",
    "        loss.backward()\n",
    "            # 3. Actualizarea tuturor ponderilor, pe baza gradientilor.\n",
    "        optim.step()\n",
    "        \n",
    "    # Calculam acuratetea\n",
    "    acc = np.sum(predictii==etichete)/len(predictii)\n",
    "    print( 'Acuratetea la epoca {} este {}%'.format(ep+1,acc*100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a3496d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################### LAB 4 ####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c8d4b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MNIST_train():\n",
    "    \n",
    "    mnist_train_data = np.zeros([60000,784]) # fiecare linie este o img 28x28\n",
    "    mnist_train_labels = np.zeros(60000)\n",
    "    \n",
    "    # ~ url = 'https://github.com/golbin/TensorFlow-MNIST/tree/master/mnist/data/train-images-idx3-ubyte.gz'\n",
    "    # ~ r = requests.get(url, allow_redirects=True)\n",
    "    # ~ open('train-images-idx3-ubyte.gz', 'wb').write(r.content)\n",
    "    \n",
    "    # ~ url = 'https://github.com/golbin/TensorFlow-MNIST/tree/master/mnist/datatrain-images-idx3-ubyte.gz'\n",
    "    # ~ r = requests.get(url, allow_redirects=True)\n",
    "    # ~ open('train-labels.idx1-ubyte.gz', 'wb').write(r.content)\n",
    "    \n",
    "    f = open('train-images.idx3-ubyte','r',encoding = 'latin-1')\n",
    "    g = open('train-labels.idx1-ubyte','r',encoding = 'latin-1')\n",
    "    \n",
    "    byte = f.read(16) #4 bytes magic number, 4 bytes nr imag, 4 bytes nr linii, 4 bytes nr coloane\n",
    "    byte_label = g.read(8) #4 bytes magic number, 4 bytes nr labels\n",
    "    \n",
    "    mnist_train_data = np.fromfile(f,dtype=np.uint8).reshape(60000,784) # modify here\n",
    "    mnist_train_labels = np.fromfile(g,dtype=np.uint8)\n",
    "        \n",
    "    # Conversii pentru a se potrivi cu procesul de antrenare    \n",
    "    mnist_train_data = mnist_train_data.astype(np.float32)\n",
    "    mnist_train_labels = mnist_train_labels.astype(np.int64)\n",
    "        \n",
    "    return mnist_train_data, mnist_train_labels\n",
    "\n",
    "def get_MNIST_test():\n",
    "    \n",
    "    mnist_test_data = np.zeros([10000,784])\n",
    "    mnist_test_labels = np.zeros(10000)\n",
    "    \n",
    "    f = open(r'Lab IV\\t10k-images.idx3-ubyte','r',encoding = 'latin-1')\n",
    "    g = open(r'Lab IV\\t10k-labels.idx1-ubyte','r',encoding = 'latin-1')\n",
    "    \n",
    "    byte = f.read(16) #4 bytes magic number, 4 bytes nr imag, 4 bytes nr linii, 4 bytes nr coloane\n",
    "    byte_label = g.read(8) #4 bytes magic number, 4 bytes nr labels\n",
    "    \n",
    "    mnist_test_data = np.fromfile(f,dtype=np.uint8).reshape(10000,784) # modify here\n",
    "    mnist_test_labels = np.fromfile(g,dtype=np.uint8)\n",
    "    \n",
    "    # Conversii pentru a se potrivi cu procesul de testare    \n",
    "    mnist_test_data = mnist_test_data.astype(np.float32)\n",
    "    mnist_test_labels = mnist_test_labels.astype(np.int64)        \n",
    "    \n",
    "    return mnist_test_data, mnist_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "041cbe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primele 200 imagini din setul de antrenare MNIST\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DatasetMNIST(Dataset):\n",
    "    def __init__(self):\n",
    "\n",
    "        mnist_train_data,mnist_train_labels = get_MNIST_train()\n",
    "\n",
    "        self.data = mnist_train_data[:200]\n",
    "        self.labels = mnist_train_labels[:200]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_point = torch.from_numpy(self.data[index, ...])\n",
    "        label_point = torch.from_numpy(self.labels[index, ...])\n",
    "\n",
    "        return {'data': data_point, 'label': label_point}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "147fd79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 straturi ascunse (3 straturi fully connected) + BatchNorm\n",
    "\n",
    "class ReteaMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    " \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        # self.drop_x = nn.Dropout(p=0.5)\n",
    "        self.bn_x = nn.BatchNorm1d(num_features = 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        # x = self.drop_x(x)\n",
    "        x = self.bn_x(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flattening\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    " \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a195e6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratetea la epoca 1 este 10.5%\n",
      "Acuratetea la epoca 2 este 17.5%\n",
      "Acuratetea la epoca 3 este 27.500000000000004%\n",
      "Acuratetea la epoca 4 este 44.5%\n",
      "Acuratetea la epoca 5 este 54.50000000000001%\n",
      "Acuratetea la epoca 6 este 69.5%\n",
      "Acuratetea la epoca 7 este 73.0%\n",
      "Acuratetea la epoca 8 este 76.0%\n",
      "Acuratetea la epoca 9 este 80.0%\n",
      "Acuratetea la epoca 10 este 83.0%\n",
      "Acuratetea la epoca 11 este 85.5%\n",
      "Acuratetea la epoca 12 este 86.5%\n",
      "Acuratetea la epoca 13 este 86.5%\n",
      "Acuratetea la epoca 14 este 89.0%\n",
      "Acuratetea la epoca 15 este 88.5%\n",
      "Acuratetea la epoca 16 este 90.0%\n",
      "Acuratetea la epoca 17 este 90.5%\n",
      "Acuratetea la epoca 18 este 92.0%\n",
      "Acuratetea la epoca 19 este 92.0%\n",
      "Acuratetea la epoca 20 este 93.0%\n",
      "Acuratetea la epoca 21 este 92.0%\n",
      "Acuratetea la epoca 22 este 93.0%\n",
      "Acuratetea la epoca 23 este 94.5%\n",
      "Acuratetea la epoca 24 este 94.0%\n",
      "Acuratetea la epoca 25 este 94.5%\n",
      "Acuratetea la epoca 26 este 94.5%\n",
      "Acuratetea la epoca 27 este 94.5%\n",
      "Acuratetea la epoca 28 este 94.0%\n",
      "Acuratetea la epoca 29 este 95.0%\n",
      "Acuratetea la epoca 30 este 94.5%\n",
      "Acuratetea la epoca 31 este 95.0%\n",
      "Acuratetea la epoca 32 este 95.0%\n",
      "Acuratetea la epoca 33 este 94.5%\n",
      "Acuratetea la epoca 34 este 94.5%\n",
      "Acuratetea la epoca 35 este 95.0%\n",
      "Acuratetea la epoca 36 este 95.5%\n",
      "Acuratetea la epoca 37 este 95.0%\n",
      "Acuratetea la epoca 38 este 95.0%\n",
      "Acuratetea la epoca 39 este 95.5%\n",
      "Acuratetea la epoca 40 este 95.5%\n"
     ]
    }
   ],
   "source": [
    "# Train MLP pe MNIST cu BatchNorm, 512 neuroni in primul strat ascuns, 128 in al doilea strat ascuns\n",
    "# 40 epoci\n",
    "\n",
    "mnistdataset = DatasetMNIST()\n",
    "mnist_loader = DataLoader(mnistdataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "simple_CNN = ReteaMLP()\n",
    "loss_function = nn.CrossEntropyLoss(reduction='sum')\n",
    "optim = torch.optim.SGD(simple_CNN.parameters(), lr=1e-4)\n",
    "\n",
    "for ep in range(40):\n",
    " \n",
    "    predictii = []\n",
    "    etichete = []\n",
    " \n",
    "    for batch in mnist_loader:\n",
    "        data, label = batch['data'], batch['label']\n",
    " \n",
    "        current_predict = simple_CNN.forward(data)\n",
    " \n",
    "        loss = loss_function(current_predict, label)\n",
    " \n",
    "        current_predict = np.argmax(current_predict.detach().numpy(), axis=1)\n",
    " \n",
    "        predictii = np.concatenate((predictii,current_predict))\n",
    "        etichete = np.concatenate((etichete,label))\n",
    " \n",
    "        optim.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer = optim, step_size = 20, gamma=0.1)\n",
    " \n",
    "    acc = np.sum(predictii==etichete)/len(predictii)\n",
    "    print( 'Acuratetea la epoca {} este {}%'.format(ep+1,acc*100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a867559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformari pentru AlexNet (augmentare + normalizare)\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "class DatasetMNIST(Dataset):\n",
    "    def __init__(self):\n",
    "\n",
    "        mnist_train_data,mnist_train_labels = get_MNIST_train()\n",
    "\n",
    "        self.data = mnist_train_data[:200]\n",
    "        self.labels = mnist_train_labels[:200]\n",
    "\n",
    "        self.transf = transforms.Compose([\n",
    "            transforms.ToPILImage(), transforms.Resize([224,224]),\n",
    "            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_point = torch.from_numpy(self.data[index, ...])\n",
    "        label_point = torch.from_numpy(self.labels[index, ...])\n",
    "\n",
    "        data_point = np.tile(data_point[..., None], (1,1,3))\n",
    "\n",
    "        data_point = self.transf(data_point)\n",
    "\n",
    "        label_point = torch.tensor(label_point,dtype=torch.long)\n",
    "\n",
    "        return {'data': data_point, 'label': label_point}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb439748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\maria\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model AlexNet pre-antrenat\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "cnn = models.alexnet(pretrained=True)\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b0b86ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in cnn.features.parameters():\n",
    "    param.requires_grad = False # nu se mai actualizeaza ponderile\n",
    "\n",
    "cnn.classifier[6] = nn.Linear(4096,10) # inlocuim ultimul strat fully-connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a33a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input type float32 is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m predictii = []\n\u001b[32m     12\u001b[39m etichete = []\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmnist_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurrent_predict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msimple_CNN\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    631\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    632\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    636\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    637\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    676\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    678\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    679\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     49\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     49\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mDatasetMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     19\u001b[39m label_point = torch.from_numpy(\u001b[38;5;28mself\u001b[39m.labels[index, ...])\n\u001b[32m     21\u001b[39m data_point = np.tile(data_point[..., \u001b[38;5;28;01mNone\u001b[39;00m], (\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m data_point = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m label_point = torch.tensor(label_point,dtype=torch.long)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m: data_point, \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m: label_point}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:234\u001b[39m, in \u001b[36mToPILImage.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    226\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[33;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    232\u001b[39m \n\u001b[32m    233\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\functional.py:335\u001b[39m, in \u001b[36mto_pil_image\u001b[39m\u001b[34m(pic, mode)\u001b[39m\n\u001b[32m    332\u001b[39m         mode = \u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnpimg.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not supported\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Image.fromarray(npimg, mode=mode)\n",
      "\u001b[31mTypeError\u001b[39m: Input type float32 is not supported"
     ]
    }
   ],
   "source": [
    "mnistdataset = DatasetMNIST()\n",
    "mnist_loader = DataLoader(mnistdataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "simple_CNN = cnn\n",
    "loss_function = nn.CrossEntropyLoss(reduction='sum')\n",
    "optim = torch.optim.SGD(simple_CNN.parameters(), lr=1e-4)\n",
    "\n",
    "# Bucla de invatare\n",
    "     \n",
    "for ep in range(40):\n",
    " \n",
    "    predictii = []\n",
    "    etichete = []\n",
    " \n",
    "    for batch in mnist_loader:\n",
    "        data, label = batch['data'], batch['label']\n",
    " \n",
    "        current_predict = simple_CNN.forward(data.float())\n",
    " \n",
    "        loss = loss_function(current_predict, label)\n",
    " \n",
    "        current_predict = np.argmax(current_predict.detach().cpu().numpy(), axis=1)\n",
    " \n",
    "        predictii = np.concatenate((predictii,current_predict))\n",
    "        etichete = np.concatenate((etichete,label))\n",
    " \n",
    "        optim.zero_grad()\n",
    " \n",
    "        loss.backward()\n",
    " \n",
    "        optim.step()\n",
    " \n",
    "    acc = np.sum(predictii==etichete)/len(predictii)\n",
    "    print( 'Acuratetea la epoca {} este {}%'.format(ep+1,acc*100) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
