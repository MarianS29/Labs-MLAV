{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning pentru Aplicatii Vizuale\n",
    "# Laborator 5: Retele Generative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducere\n",
    "\n",
    "In laboratoarele anterioare, scopul retelelor neuronale antrenate a fost de a estima probabilitatea $P(y|x), y \\in C$, unde $y$ este o eticheta din totalitatea claselor $C$, iar $x \\in X$ este imaginea curenta (cunoscuta). Din acest motiv, aceasta clasa de arhitecturi sunt denumite <b>modele discriminative</b>. In contrast cu acestea, <b>modelele generative</b> incearca sa determine un posibil $x$ apartinand uneia dintre clasele $y$ sau, mai exact, in cazul aplicatiilor vizuale, pixelii unei imagini. Plecand de la un set de date oarecare, se doreste obtinerea unor esantioane noi, care ar putea face parte din acel set. De exemplu, pentru baza de date MNIST, generatorul poate incerca sa sintetizeze imagini noi cu fiecarea cifra, cum ar fi \"5\".\n",
    "\n",
    "Desi modelele generative nu sunt o noutate in <i>Machine Learning</i>, acestea au prins un avant substantial odata cu aparitia unor solutii moderne performante, precum VAE (<i>Variational Autoencoders</i>) si <b>GAN</b>. In continuare, vor fi explorate ultimele, in implementarea lor bazata pe CNN-uri. Plecand de la setul de date <b>MNIST</b>, folosit anterior in sarcina tipica de clasificare, va fi prezentata arhitectura generala a unui GAN si diferentele fata de modul de antrenare a retelelor anterioare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retele Generative Adversariale\n",
    "\n",
    "### 2.1. Principii fundamentale\n",
    "\n",
    "Modelele bazate pe Retele Generative Adversariale (<i>Generative Adversarial Nets</i> - GAN) se disting pe utilizarea simultana a doua arhitecturi neuronale: una denumita generator (<b>G</b>) si un discriminator (<b>D</b>). Aceste doua arhitecturi se afla in opozitie una fata de cealalta pe parcursul procesului de antrenare, performanta fiecareia bazandu-se pe evolutia celeilalte. Reteaua D trebuie sa clasifice un esantion din setul de date de antrenare ca fiind real iar unul generat de G ca fiind fals. In acelasi timp G incearca sa creeze esantioane noi cat mai credibile, care sa fie clasificate de D ca fiind reale. \n",
    "\n",
    "Daca pentru arhitectura D intrarea se poate deduce ca este de forma datelor reale (ex.: imagini), pentru generator, lucrurile nu mai sunt atat de clare. Punctul de start pentru acesta este un vector de valori aleatoare esantionate dintr-o distributie aleasa, denumit vector latent (sau un <i>zgomot</i>) $z$. Tinand cont de modul in care functioneaza algoritmul, nu exista informatii despre cum influenteaza valorile din vectorul $z$ iesirea lui G.\n",
    "\n",
    "Formal, din punct de vedere al procesului de optimizare, loss-ul a fost definit in lucrarea originala (\\[1\\]) ca:\n",
    "$$\\underset{G}{min}\\underset{D}{max}L(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}[logD(x)] + \\mathbb{E}_{z\\sim p_{z}(z)}[log(1-D(G(z)))]$$\n",
    "\n",
    "Desi loss-ul de mai sus este intuitiv, fiind o combinatie de costuri de tipul Entropie Incrucisata Binara (<i>Binary Cross Entropy</i>), in practica antrenarea unei retele GAN de acest fel este un proces sensibil la diverse variatii aleatoare, predispus la instabilitate. Daca in varianta originala s-au folosit MLP-uri pentru retelele D si G, procesul de generare folosind retele convolutionale a fost descris in \\[2\\], iar alte costuri precum cel Wasserstein (\\[3\\]) au adus imbunatatiri substantiale stabilitatii si calitatii rezultatelor finale. Cu toate acestea, pentru a nu complica lucrurile, in partea practica a acestei lucrari de laborator se vor folosi arhitecturi convolutionale antrenate cu o functie loss similara celei de mai sus.\n",
    "\n",
    "Particularizand pentru aplicatiile vizuale, scopul unui GAN este de a genera imagini noi, care sa fie similare din punct de vedere vizual cu un set de imagini naturale (reale). Pentru a ilustra acest fapt, s-a urmarit evolutia retelei G la intrarea careia s-a aplicat acelasi vector $z$ pe parcursul procesului de antrenare. La fiecare 100 de iteratii, imaginea obtinuta a fost comparata cu toate esantioanele din setul de antrenare MNIST, folosind o metrica de similaritate structurala (<i>Structural Similarity Index Measure</i> - SSIM). In graficul de mai jos este afisata valoarea medie a SSIM pe parcursul a 1800 de iteratii. Se poate observa ca dupa o perioada initiala instabila, imaginea generata a inceput sa fie asemanatoare cu cele folosite la antrenare.\n",
    "\n",
    "<center><img src=\"media/comp_ssim.png\" width=\"500\"></center>\n",
    "<center>Evolutia SSIM pe parcursul unei antrenari. La fiecare 100 de iteratii a fost calculata valoarea medie a SSIM,<br> comparand rezultatul lui G pentru acelasi vector de intrare $z$ cu imaginile de antrenare din MNIST. </center><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Reteaua discriminatoare (D)\n",
    "\n",
    "Reteaua D care va fi folosita in aceasta lucrare este similara conceptual celei folosite in laboratoarele anterioare. Plecand de la o imagine 28 x 28 pixeli cu un singur canal de culoare, reteaua trece prin mai multe straturi de tip ```nn.Conv2D``` iar iesirea consta intr-un strat ```nn.Linear```. De aceasta data, insa, in loc de zece neuroni de iesire (corespunzatori celor 10 cifre), reteaua va avea doar 2 valori, deoarece problema de clasificare se transforma din <i>care din cele zece cifre este prezenta in imagine</i>, in <i>este reala sau generata imaginea</i>. Din aceasta schimbare a problemei, se poate deduce faptul ca nu conteaza care din cifre este generata, ci daca este suficient de convigatoare noua imagine.\n",
    "\n",
    "Alte diferente importante fata de arhitecturile anterioare este lipsa straturilor de tip ```nn.MaxPool2d``` si folosirea altei functii de activare. Pentru a reduce numarul de linii si coloane ale hartilor de activare, se vor folosi convolutii cu pas (<i>stride</i>) mai mare de 1. Din perspectiva functiei de activare, in loc de ReLU se va folosi <i>Leaky ReLU</i>. Aceasta va permite si valori negative, insa atenuate cu un factor constant, care trebuie specificat: $$ LeakyReLU(x) = \\begin{cases} x, & x >= 0\\\\ atenuare * x, & x < 0 \\end{cases}$$\n",
    "\n",
    "<center><img src=\"media/leaky_relu.png\" width=\"500\"></center>\n",
    "<center>Functia de activare Leaky ReLU, atunci cand factorul de atenuare este setat la 0.05</center><br><br>\n",
    "\n",
    "Modificarile fata de arhitecturile anterioare au fost facute pe baza recomandarilor din \\[2\\], pentru a asigura o stabilitate mai buna a procesului de antrenare al ansamblului. Clasa arhitecturii de tip discrimnator este deci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Discriminator code\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Intrare imagine reala - nr_imag x 1 x 128 x 128\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.lrelu1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        # nr_imag x 16 x 14 x 14\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.lrelu2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        # nr_imag x 32 x 7 x 7\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.lrelu3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        # nr_imag x 64 x 4 x 4\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.lrelu4 = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        # nr_imag x 128 x 1 x 1\n",
    "        self.out = nn.Linear(in_features=128, out_features=2)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.conv1(input)\n",
    "        x = self.lrelu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.lrelu2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lrelu3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.lrelu4(x)\n",
    "\n",
    "        x = torch.flatten(x, 1, 3)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(pentru simplitate, au fost adaugate si toate bibliotecile utilizate in restul codului)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Reteaua generatoare\n",
    "\n",
    "Spre deosebire de arhitectura de mai sus, reteaua G are un comportament vizibil diferit. Pana acum, s-a plecat de la o imagine, iar reprezentarile intermediare au avut o rezolutie din ce in ce mai mica. In acest caz, insa, intrarea este un vector, dar rezolutia reprezentarii va creste pe parcursul propagarii inainte, rezultand o imagine. Acest lucru este realizat cu ajutorul unei noi operatii, denumita <i>convolutie transpusa</i> (intalnita uneori, in mod eronat, ca deconvolutie). Cel mai simplu mod de a urmari schimbarea dimensionalitatii este de a inversa intrarea si iesirea din strat. Daca pe matricea $X$ se aplica o convolutie cu un nucleu de dimensiune $k$, pas $p$, etc. se obtine iesirea $Y$, a carei dimensiuni se pot calcula. In cazul convolutiei transpuse, avand aceiasi parametri $k$, $p$, etc., se pune intrebarea \"ce dimensiune ar trebui sa aiba $Y$, astfel incat sa se obtina o harta de activare de dimensiunea $X$, daca s-ar aplica convolutia pe $Y$?\".\n",
    "\n",
    "Din aceasta modalitate inversa de a urmari dimensionalitatea matricilor, variabilele pe care le-am discutat in cadrul convolutiilor ajung sa aiba un efect opus fata de cel obisnuit. Adaugarea bordarii va conduce la o iesire mai mica, iar cresterea pasului va duce la o matrice de iesire mai mare. In figura de mai jos se pot urmari cateva exemple ale acestui comportament. Pentru o prezentare in detaliu a acestui tip de strat, este recomandata citirea referintei \\[4\\].\n",
    "\n",
    "<center>\n",
    "<video align=\"center\" width=\"344\" height=\"386\" controls><source src=\"media/[proc]conv_transp_no_pad_no_stride.mp4\" type=\"video/mp4\"></video>\n",
    "<video align=\"center\" width=\"344\" height=\"386\" controls><source src=\"media/conv_transp_same_pad_no_stride.mp4\" type=\"video/mp4\"></video>\n",
    "</center>\n",
    "<center>(stanga) Convolutie transpusa fara bordare si pas unitar<br>\n",
    "(dreapta) Convolutie transpusa cu bordare si pas unitar<br>\n",
    "Animatii preluate din <a href=\"https://github.com/vdumoulin/conv_arithmetic\">aceasta resursa</a></center>\n",
    "\n",
    "Pentru a defini un strat de convolutie transpusa, se utilizeaza urmatoarea sintaxa:\n",
    "\n",
    "```tconv_x = nn.ConvTranspose2d(in_channels = nr_canale_input, out_channels = nr_canale_output, kernel_size = dim_filtru, stride = pas, padding = dim_bordare, output_padding = dim_bordare_iesire, dilation=dilatare)```\n",
    "\n",
    "Pentru a calcula rapid dimensiunea iesirii din acest strat se foloseste urmatoarea formula, din documentatia oficiala <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\">PyTorch</a> (cu o varianta echivalenta si pentru latime):\n",
    "\n",
    "$$H_{out} = (H_{in}-1) * pas - 2 * dim\\_bordare + dilatare * (dim\\_filtru-1) + dim\\_bordare\\_iesire + 1$$\n",
    "\n",
    "In mod obisnuit, nu se aplicau functii de activare pe iesirea din retea, deoarece functia de cost ```nn.CrossEntropyLoss``` aplica implicit o activare Softmax. Acum insa, se doreste ca valorile de iesire din retea sa fie dintr-un interval inchis adecvat pentru lucrul cu imagini. Din acest motiv reteaua G are drept ultim strat o activare de tip Sigmoid, care restrange intervalul de valori la $(0, 1)$.\n",
    "\n",
    "<b>!Atentie</b>: Trebuie reamintit ca, desi imaginile au cel mai des valori intregi in intervalul $[0,255]$, atunci cand se lucreaza cu valori reale, multe biblioteci necesita ca intervalul sa fie $[0,1]$, fapt ce justifica suplimentar alegerea activarii sigmoide.\n",
    "\n",
    "Clasa arhitecturii generatoare este in acest caz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, dim_zg):\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Intrare vector latent - nr_imag x 100 x 1 x 1\n",
    "        self.tconv1 = nn.ConvTranspose2d( in_channels=dim_zg, out_channels=128, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.relu1 = nn.ReLU(True)\n",
    "        \n",
    "        # nr_imag x 128 x 4 x 4\n",
    "        self.tconv2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU(True)\n",
    "\n",
    "        # nr_imag x 64 x 8 x 8\n",
    "        self.tconv3 = nn.ConvTranspose2d( in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.relu3 = nn.ReLU(True)\n",
    "\n",
    "        # nr_imag x 32 x 16 x 16\n",
    "        self.tconv4 = nn.ConvTranspose2d( in_channels=32, out_channels=1, kernel_size=4, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # nr_imag x 1 x 28 x 28\n",
    "        self.out = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.tconv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.tconv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.tconv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.tconv4(x)\n",
    "\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Schimbari ale clasei Dataset\n",
    "\n",
    "Clasa derivata din ```Dataset``` are in mare parte aceeasi structura cu varianta folosita in laboratoarele anterioare. Totusi, deoarece discriminatorul este interesat doar distinctia dintre imagini <b>reale</b> si imagini <b>generate</b>, nu mai este nevoie sa fie incarcate etichetele din setul de date MNIST. De asemenea, valorile pixelilor imaginilor sunt impartite la $255$, reducand intervalul de valori la $[0,1]$. Aceasta alegere poate fi explicata o data de faptul ca si imaginile generate se afla in acest interval, dar si de faptul ca, in general, aceasta plaja de valori asigura o performanta mai mare a retelei D. Clasa devine astfel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMNIST_GAN(Dataset):\n",
    "    def __init__(self, cale_catre_date):\n",
    "\n",
    "        f = open(cale_catre_date,'r',encoding = 'latin-1')\n",
    "\n",
    "        byte = f.read(16) #4 bytes magic number, 4 bytes nr imag, 4 bytes nr linii, 4 bytes nr coloane\n",
    "\n",
    "        mnist_data = np.fromfile(f,dtype=np.uint8).reshape(-1,1,28,28)\n",
    "\n",
    "        # Conversii pentru a se potrivi cu procesul de antrenare    \n",
    "        self.mnist_data = mnist_data.astype(np.float32)/255\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mnist_data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "            \n",
    "        date = self.mnist_data[idx,:,:,:]     \n",
    "        \n",
    "        return date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Antrenarea unui GAN\n",
    "\n",
    "Dupa definirea retelelor si a clasei derivate din ```Dataset```, acestea se pot instantia si sa fie scrisa bucla principala de antrenare. In cazul fiecarei iteratii de antrenare se vor distinge doua zone distincte:\n",
    "- antrenarea discriminatorului D, unde\n",
    "  - sunt propagate prin D imaginile reale din batch-ul curent\n",
    "  - se genereaza vectorii de intrare in G ca ```torch.randn(len(data), dim_zg, 1, 1)``` (se observa ca trebuie pastrata structura de matrice 4D pentru stratul de convolutie transpusa)\n",
    "  - se genereaza imagini plecand de la vectorii de mai sus\n",
    "  - sunt propagate imaginile generate de G prin D, atasandu-le eticheta de imagini generate (false - 0)\n",
    "  - se actualizeaza ponderile lui D\n",
    "- antrenarea generatorului G, unde\n",
    "  - aceleasi imagini generate de G sunt propagate prin D, de data aceasta atasandu-le eticheta de imagini reale (1)\n",
    "  - se actualizeaza ponderile lui G, in functie de cat de bine a \"pacalit\" pe D; un valoare mica a loss-ului inseamna ca discriminatorul a considerat multe din imagini ca fiind reale\n",
    "  \n",
    "Pe langa aceasta schimbare importanta in modul de antrenare, se va mai observa un set de vectori denumiti ```esantioane_proba```, care sunt propagati prin G la sfarsitul fiecarei epoci. Desi nu are vreun rol in antrenarea arhitecturii, in acest mod se poate urmari evolutia ei de-a lungul iteratiilor.\n",
    "\n",
    "Deoarece epocile ar trebui sa dureze mai mult in aceasta antrenare, a fost utilizata biblioteca ```tqdm``` pentru a urmari evolutia iteratiilor. Pentru instalare este suficienta rularea comenzii ```pip install tqdm``` din linia de comanda.\n",
    "\n",
    "Codul de antrenare este deci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setare seed pentru reproductibilitatea experimentului\n",
    "seed = 999\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Number of training epochs\n",
    "nr_epoci = 5\n",
    "\n",
    "# Initializare set de date si dataloader\n",
    "dataset = DatasetMNIST_GAN(r'train-images.idx3-ubyte')\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Dimensiunea vectorului latent (zgomotul folosit la generare)\n",
    "dim_zg = 100\n",
    "# Initializare retele\n",
    "retea_g = Generator(dim_zg)\n",
    "retea_d = Discriminator()\n",
    "\n",
    "# Functia loss\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Esantioane din care se vor genera imagini, pentru a vizualiza imbunatatirea retelei pe parcursul antrenarii\n",
    "esantioane_proba = torch.randn(64, dim_zg, 1, 1)\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(retea_d.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(retea_g.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "# Lista de imagini obtinute din esantioane_proba\n",
    "img_list = []\n",
    "\n",
    "# Bucla principala de antrenare\n",
    "# Imaginile reale au eticheta 1\n",
    "# Imaginile false au eticheta 0\n",
    "for ep in range(nr_epoci):\n",
    "    for data in tqdm(dataloader):\n",
    "\n",
    "        # Antrenarea retelei discriminatoare -> se doreste ca reteaua aceasta sa identifice esantioanele\n",
    "        # generate ca fiind false si pe cele din setul de date ca fiind reale\n",
    "\n",
    "        # Imagini reale\n",
    "        retea_d.zero_grad()\n",
    "\n",
    "        # Eticheta reala este 1\n",
    "        # Se genereaza un vector de intregi de valoare 1\n",
    "        label = torch.LongTensor(np.ones(len(data)))\n",
    "        output = retea_d(data)\n",
    "        loss_imagini_reale = loss_function(output, label.long())\n",
    "        loss_imagini_reale.backward()\n",
    "\n",
    "        # Imagini generate\n",
    "        # Generare vector latent\n",
    "        vector_generare = torch.randn(len(data), dim_zg, 1, 1)\n",
    "        # Generare imagini din vectorul latent\n",
    "        imagini_generate = retea_g(vector_generare)\n",
    "        # Eticheta imaginilor generate este 0\n",
    "        label = torch.LongTensor(np.zeros(len(data)))\n",
    "        # Clasificare imaginilor generate\n",
    "        # Nu se doreste ajustarea ponderilor generatorului acum, motiv pentru \n",
    "        # care se foloseste metoda detach care asigura ca nu se vor calcula \n",
    "        # gradientii pentru G\n",
    "        output = retea_d(imagini_generate.detach())\n",
    "        loss_imagini_generate = loss_function(output, label.long())\n",
    "        loss_imagini_generate.backward()\n",
    "\n",
    "        # Ajustarea ponderilor discriminatorului\n",
    "        optimizerD.step()\n",
    "\n",
    "\n",
    "        # Antrenarea retelei generatoare -> se doreste ca reteaua discriminatoare sa identifice esantioanele\n",
    "        # generate ca fiind reale\n",
    "\n",
    "        retea_g.zero_grad()\n",
    "        # Pentru a antrena generatorul, reteaua discriminatoare trebuie sa vada imaginile generate ca fiind reale\n",
    "        label = torch.LongTensor(np.ones(len(data)))\n",
    "        output = retea_d(imagini_generate)\n",
    "        loss_imagini_generate = loss_function(output, label.long())\n",
    "        loss_imagini_generate.backward()\n",
    "        # Ajustarea ponderilor generatorului\n",
    "        optimizerG.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        imagini_generate = retea_g(esantioane_proba).detach()\n",
    "    img_list.append(vutils.make_grid(imagini_generate, padding=2, normalize=True))\n",
    "\n",
    "    print('Epoca {} a fost incheiata'.format(ep+1))\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())\n",
    "\n",
    "# Afisarea ultimelor imagini de proba generate\n",
    "plt.figure()\n",
    "plt.title(\"Imagini generate\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>!Atentie</b>: in mod normal, pachetul IPython ar trebui sa fie instalat, daca ati lucrat anterior cu Jupyter Notebook. Daca acesta lipseste, totusi, se poate instala cu ajutorul pip - ```pip install ipython```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercitiu: Antrenati o arhitectura GAN care sa genereze cifre, folosind setul MNIST ca sursa de date reale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercitiu: Modificati codul din exercitiul anterior, pentru a genera doar cifra 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliografie\n",
    "1. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27.\n",
    "2. Radford, A., Metz, L., & Chintala, S. (2015). Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434.\n",
    "3. Arjovsky, M., Chintala, S., & Bottou, L. (2017, July). Wasserstein generative adversarial networks. In International conference on machine learning (pp. 214-223). PMLR.\n",
    "4. Dumoulin, V., & Visin, F. (2016). A guide to convolution arithmetic for deep learning. arXiv preprint arXiv:1603.07285."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
